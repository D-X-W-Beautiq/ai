{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fNW-dCMBJy8W",
        "p6oxLWsmJsb8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### blip ì‚¬ìš©í•´ì„œ ìº¡ì…˜ ë§Œë“¤ê¸°"
      ],
      "metadata": {
        "id": "bQYUjDfnJ5-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk1_Vq1eJklo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, Blip2ForImageTextRetrieval\n",
        "\n",
        "# ====== ì„¤ì • ======\n",
        "image_base_dir = \"/content/drive/MyDrive/ì»¨í¼ëŸ°ìŠ¤/ìŠ¤íƒ€ì¼ì¶”ì²œ/í•œêµ­ì¸ì–¼êµ´/image_celeb/image_celeb/image_cropped\"\n",
        "output_dir = \"/content/drive/MyDrive/korean_celebrity_makeup_captions\"\n",
        "batch_size = 50\n",
        "\n",
        "# ì¶œë ¥ í´ë” ìƒì„±\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ====== í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì •ì˜ ======\n",
        "categories = {\n",
        "    \"ë¶„ìœ„ê¸°\": [\n",
        "        \"ê¾¸ì•ˆê¾¸\", \"ë‚´ì¶”ëŸ´\", \"ë°ì¼ë¦¬\", \"ì„¸ë ¨ë¨\", \"ëª¨ë˜\", \"ì²­ìˆœ\", \"ëŸ¬ë¸”ë¦¬\", \"ì°¨ë¶„\",\n",
        "        \"ì‹œí¬\", \"íŠ¸ë Œë””\", \"ìš°ì•„\", \"ëŸ­ì…”ë¦¬\", \"ë°œë„\", \"ì²­ëŸ‰\", \"í˜ë¯¸ë‹Œ\", \"ì„¹ì‹œ\",\n",
        "        \"ì‹¬í”Œ\", \"ë°ì€\", \"ì€ì€í•œ\", \"ê¹”ë”\", \"ê³ ê¸‰ìŠ¤ëŸ¬ìš´\", \"ì¿¨\", \"ë§¤í˜¹ì \",\n",
        "        \"ê·€ì—¬ìš´\", \"ê¹œì°\", \"ì‚¬ë‘ìŠ¤ëŸ¬ìš´\", \"í™œê¸°ì°¬\", \"ìƒí¼\", \"ì„±ìˆ™\", \"í´ë˜ì‹\"\n",
        "    ],\n",
        "    \"ëˆˆ\": [\n",
        "        \"ë‚´ì¶”ëŸ´ì•„ì´\", \"ìŠ¤ëª¨í‚¤ì•„ì´\", \"ì„¸ë¯¸ìŠ¤ëª¨í‚¤\", \"ê¸€ë¦¬í„°ì•„ì´\", \"ë§¤íŠ¸ì•„ì´\",\n",
        "        \"ë¸Œë¼ìš´ì•„ì´\", \"í•‘í¬ì•„ì´\", \"ì½”ë„ì•„ì´\", \"ì˜¤ë Œì§€ì•„ì´\",\n",
        "        \"ì–¸ë”ë¼ì¸\", \"ë˜ë ·í•œë¼ì¸\", \"ìì—°ìŠ¤ëŸ¬ìš´ë¼ì¸\", \"ìŒì˜\"\n",
        "    ],\n",
        "    \"í”¼ë¶€\": [\n",
        "        \"ê´‘ì±„í”¼ë¶€\", \"ë¬¼ê´‘í”¼ë¶€\", \"ìœ¤ê¸°í”¼ë¶€\", \"ë§¤íŠ¸í”¼ë¶€\", \"ë³´ì†¡í”¼ë¶€\", \"ìˆ˜ë¶„í”¼ë¶€\",\n",
        "        \"ì´‰ì´‰í”¼ë¶€\", \"íˆ¬ëª…í”¼ë¶€\", \"ê¸€ë¡œìš°\", \"í•˜ì´ë¼ì´íŠ¸\", \"ì»¨íˆ¬ì–´\", \"ì‰ë”©\", \"í†¤ì—…\"\n",
        "    ],\n",
        "    \"ë¸”ëŸ¬ì…”\": [\n",
        "        \"í•‘í¬ë¸”ëŸ¬ì…”\", \"ì½”ë„ë¸”ëŸ¬ì…”\", \"í”¼ì¹˜ë¸”ëŸ¬ì…”\", \"ì˜¤ë Œì§€ë¸”ëŸ¬ì…”\"\n",
        "    ],\n",
        "    \"ì…ìˆ \": [\n",
        "        \"ë ˆë“œë¦½\", \"í•‘í¬ë¦½\", \"ì½”ë„ë¦½\", \"ëˆ„ë“œë¦½\", \"ë¸Œë¼ìš´ë¦½\", \"ì˜¤ë Œì§€ë¦½\", \"ë¡œì¦ˆë¦½\",\n",
        "        \"ë§¤íŠ¸ë¦½\", \"ê¸€ë¡œì‹œë¦½\", \"ê·¸ë¼ë°ì´ì…˜ë¦½\", \"ìì—°ìŠ¤ëŸ¬ìš´ë¦½\", \"ë˜ë ·í•œë¦½ë¼ì¸\"\n",
        "    ],\n",
        "    \"ëˆˆì¹\": [\n",
        "        \"ìì—°ìŠ¤ëŸ¬ìš´ëˆˆì¹\", \"ì•„ì¹˜ëˆˆì¹\", \"ì¼ìëˆˆì¹\", \"ì§„í•œëˆˆì¹\", \"ì—°í•œëˆˆì¹\"\n",
        "    ],\n",
        "    \"ì†ëˆˆì¹\": [\n",
        "        \"ìì—°ìŠ¤ëŸ¬ìš´ì†ëˆˆì¹\", \"ë³¼ë¥¨ì†ëˆˆì¹\", \"ê¸´ì†ëˆˆì¹\", \"ì»¬ë§ì†ëˆˆì¹\", \"ë³¼ë¥¨ë§ˆìŠ¤ì¹´ë¼\", \"ë¡±ë˜ì‰¬ë§ˆìŠ¤ì¹´ë¼\"\n",
        "    ],\n",
        "    \"í†¤\": [\n",
        "        \"ì›œí†¤\", \"ì¿¨í†¤\", \"ë‰´íŠ¸ëŸ´í†¤\", \"ë´„ì›œ\", \"ì—¬ë¦„ì¿¨\", \"ê°€ì„ì›œ\", \"ê²¨ìš¸ì¿¨\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# í•œì˜ ë²ˆì—­ ë”•ì…”ë„ˆë¦¬\n",
        "keyword_translation = {\n",
        "    # ë¶„ìœ„ê¸°\n",
        "    \"ê¾¸ì•ˆê¾¸\": \"effortless style\", \"ë‚´ì¶”ëŸ´\": \"natural beauty\", \"ë°ì¼ë¦¬\": \"everyday look\",\n",
        "    \"ì„¸ë ¨ë¨\": \"sophisticated style\", \"ëª¨ë˜\": \"modern style\", \"ì²­ìˆœ\": \"innocent beauty\",\n",
        "    \"ëŸ¬ë¸”ë¦¬\": \"lovely charm\", \"ì°¨ë¶„\": \"calm elegance\", \"ì‹œí¬\": \"chic style\",\n",
        "    \"íŠ¸ë Œë””\": \"trendy look\", \"ìš°ì•„\": \"elegant style\", \"ëŸ­ì…”ë¦¬\": \"luxury look\",\n",
        "    \"ë°œë„\": \"bright energy\", \"ì²­ëŸ‰\": \"fresh beauty\", \"í˜ë¯¸ë‹Œ\": \"feminine beauty\",\n",
        "    \"ì„¹ì‹œ\": \"sexy appeal\", \"ì‹¬í”Œ\": \"simple style\", \"ë°ì€\": \"bright mood\",\n",
        "    \"ì€ì€í•œ\": \"subtle beauty\", \"ê¹”ë”\": \"clean look\", \"ê³ ê¸‰ìŠ¤ëŸ¬ìš´\": \"luxurious style\",\n",
        "    \"ì¿¨\": \"cool style\", \"ë§¤í˜¹ì \": \"alluring charm\", \"ê·€ì—¬ìš´\": \"cute style\",\n",
        "    \"ê¹œì°\": \"adorable look\", \"ì‚¬ë‘ìŠ¤ëŸ¬ìš´\": \"lovable beauty\", \"í™œê¸°ì°¬\": \"energetic look\",\n",
        "    \"ìƒí¼\": \"fresh charm\", \"ì„±ìˆ™\": \"mature elegance\", \"í´ë˜ì‹\": \"classic style\",\n",
        "\n",
        "    # ëˆˆ\n",
        "    \"ë‚´ì¶”ëŸ´ì•„ì´\": \"natural eyes\", \"ìŠ¤ëª¨í‚¤ì•„ì´\": \"smoky eyes\", \"ì„¸ë¯¸ìŠ¤ëª¨í‚¤\": \"semi-smoky eyes\",\n",
        "    \"ê¸€ë¦¬í„°ì•„ì´\": \"glitter eyes\", \"ë§¤íŠ¸ì•„ì´\": \"matte eyes\", \"ë¸Œë¼ìš´ì•„ì´\": \"brown eyeshadow\",\n",
        "    \"í•‘í¬ì•„ì´\": \"pink eyeshadow\", \"ì½”ë„ì•„ì´\": \"coral eyeshadow\", \"ì˜¤ë Œì§€ì•„ì´\": \"orange eyeshadow\",\n",
        "    \"ì–¸ë”ë¼ì¸\": \"lower eyeliner\", \"ë˜ë ·í•œë¼ì¸\": \"defined eyeliner\", \"ìì—°ìŠ¤ëŸ¬ìš´ë¼ì¸\": \"natural eyeliner\",\n",
        "    \"ìŒì˜\": \"eye shadow depth\",\n",
        "\n",
        "    # í”¼ë¶€\n",
        "    \"ê´‘ì±„í”¼ë¶€\": \"glowing skin\", \"ë¬¼ê´‘í”¼ë¶€\": \"dewy glass skin\", \"ìœ¤ê¸°í”¼ë¶€\": \"radiant skin\",\n",
        "    \"ë§¤íŠ¸í”¼ë¶€\": \"matte skin\", \"ë³´ì†¡í”¼ë¶€\": \"soft velvet skin\", \"ìˆ˜ë¶„í”¼ë¶€\": \"hydrated skin\",\n",
        "    \"ì´‰ì´‰í”¼ë¶€\": \"moist skin\", \"íˆ¬ëª…í”¼ë¶€\": \"clear skin\", \"ê¸€ë¡œìš°\": \"glow effect\",\n",
        "    \"í•˜ì´ë¼ì´íŠ¸\": \"highlight\", \"ì»¨íˆ¬ì–´\": \"contour\", \"ì‰ë”©\": \"shading\", \"í†¤ì—…\": \"tone-up effect\",\n",
        "\n",
        "    # ë¸”ëŸ¬ì…”\n",
        "    \"í•‘í¬ë¸”ëŸ¬ì…”\": \"pink blush\", \"ì½”ë„ë¸”ëŸ¬ì…”\": \"coral blush\",\n",
        "    \"í”¼ì¹˜ë¸”ëŸ¬ì…”\": \"peach blush\", \"ì˜¤ë Œì§€ë¸”ëŸ¬ì…”\": \"orange blush\",\n",
        "\n",
        "    # ì…ìˆ \n",
        "    \"ë ˆë“œë¦½\": \"red lips\", \"í•‘í¬ë¦½\": \"pink lips\", \"ì½”ë„ë¦½\": \"coral lips\",\n",
        "    \"ëˆ„ë“œë¦½\": \"nude lips\", \"ë¸Œë¼ìš´ë¦½\": \"brown lips\", \"ì˜¤ë Œì§€ë¦½\": \"orange lips\",\n",
        "    \"ë¡œì¦ˆë¦½\": \"rose lips\", \"ë§¤íŠ¸ë¦½\": \"matte lips\", \"ê¸€ë¡œì‹œë¦½\": \"glossy lips\",\n",
        "    \"ê·¸ë¼ë°ì´ì…˜ë¦½\": \"gradient lips\", \"ìì—°ìŠ¤ëŸ¬ìš´ë¦½\": \"natural lips\", \"ë˜ë ·í•œë¦½ë¼ì¸\": \"defined lip line\",\n",
        "\n",
        "    # ëˆˆì¹\n",
        "    \"ìì—°ìŠ¤ëŸ¬ìš´ëˆˆì¹\": \"natural brows\", \"ì•„ì¹˜ëˆˆì¹\": \"arched brows\", \"ì¼ìëˆˆì¹\": \"straight brows\",\n",
        "    \"ì§„í•œëˆˆì¹\": \"bold brows\", \"ì—°í•œëˆˆì¹\": \"light brows\",\n",
        "\n",
        "    # ì†ëˆˆì¹\n",
        "    \"ìì—°ìŠ¤ëŸ¬ìš´ì†ëˆˆì¹\": \"natural lashes\", \"ë³¼ë¥¨ì†ëˆˆì¹\": \"volume lashes\", \"ê¸´ì†ëˆˆì¹\": \"long lashes\",\n",
        "    \"ì»¬ë§ì†ëˆˆì¹\": \"curled lashes\", \"ë³¼ë¥¨ë§ˆìŠ¤ì¹´ë¼\": \"volume mascara\", \"ë¡±ë˜ì‰¬ë§ˆìŠ¤ì¹´ë¼\": \"lengthening mascara\",\n",
        "\n",
        "    # í†¤\n",
        "    \"ì›œí†¤\": \"warm tone\", \"ì¿¨í†¤\": \"cool tone\", \"ë‰´íŠ¸ëŸ´í†¤\": \"neutral tone\",\n",
        "    \"ë´„ì›œ\": \"spring warm\", \"ì—¬ë¦„ì¿¨\": \"summer cool\", \"ê°€ì„ì›œ\": \"autumn warm\", \"ê²¨ìš¸ì¿¨\": \"winter cool\"\n",
        "}\n",
        "\n",
        "# ====== BLIP2 ëª¨ë¸ ì´ˆê¸°í™” ======\n",
        "def initialize_blip2_model():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "    model_name = \"Salesforce/blip2-itm-vit-g\"\n",
        "    print(f\"ğŸ¤– BLIP2 ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}\")\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "    model = Blip2ForImageTextRetrieval.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"âœ… BLIP2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
        "    return processor, model, device\n",
        "\n",
        "# ====== ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ======\n",
        "def collect_all_image_paths(base_dir):\n",
        "    image_paths = []\n",
        "\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')) and not file.startswith('._'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                image_paths.append(full_path)\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "# ====== ìœ ì‚¬ë„ ê³„ì‚° ======\n",
        "def calculate_category_similarities(image, processor, model, device, categories, keyword_translation):\n",
        "    category_results = {}\n",
        "\n",
        "    for cat_name, korean_keywords in categories.items():\n",
        "        english_texts = []\n",
        "        valid_korean_keywords = []\n",
        "\n",
        "        for korean_kw in korean_keywords:\n",
        "            if korean_kw in keyword_translation:\n",
        "                english_texts.append(keyword_translation[korean_kw])\n",
        "                valid_korean_keywords.append(korean_kw)\n",
        "\n",
        "        if not english_texts:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            inputs = processor(\n",
        "                images=image,\n",
        "                text=english_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits_per_image = outputs.logits_per_image\n",
        "                probs = logits_per_image.softmax(dim=1)[0]\n",
        "\n",
        "            best_idx = torch.argmax(probs).item()\n",
        "            best_korean_keyword = valid_korean_keywords[best_idx]\n",
        "            best_english_keyword = english_texts[best_idx]\n",
        "            best_score = probs[best_idx].item()\n",
        "\n",
        "            category_results[cat_name] = {\n",
        "                \"korean_keyword\": best_korean_keyword,\n",
        "                \"english_keyword\": best_english_keyword,\n",
        "                \"score\": best_score\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ [{cat_name}] ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
        "            continue\n",
        "\n",
        "    return category_results\n",
        "\n",
        "# ====== ìº¡ì…˜ ìƒì„± ======\n",
        "def generate_makeup_captions(category_results):\n",
        "    detail_categories = [\"ëˆˆ\", \"í”¼ë¶€\", \"ë¸”ëŸ¬ì…”\", \"ì…ìˆ \", \"ëˆˆì¹\", \"ì†ëˆˆì¹\"]\n",
        "\n",
        "    detail_results = []\n",
        "    for cat in detail_categories:\n",
        "        if cat in category_results:\n",
        "            detail_results.append({\n",
        "                \"category\": cat,\n",
        "                \"korean_keyword\": category_results[cat][\"korean_keyword\"],\n",
        "                \"english_keyword\": category_results[cat][\"english_keyword\"],\n",
        "                \"score\": category_results[cat][\"score\"]\n",
        "            })\n",
        "\n",
        "    detail_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    top_2_details = detail_results[:2]\n",
        "\n",
        "    captions = {}\n",
        "\n",
        "    # ìº¡ì…˜ íƒ€ì… 1: ë¶„ìœ„ê¸° 1ê°œ + ë©”ì´í¬ì—… ë””í…Œì¼ 2ê°œ\n",
        "    if \"ë¶„ìœ„ê¸°\" in category_results and len(top_2_details) >= 2:\n",
        "        mood = category_results[\"ë¶„ìœ„ê¸°\"]\n",
        "        detail1 = top_2_details[0]\n",
        "        detail2 = top_2_details[1]\n",
        "\n",
        "        captions[\"caption_type1\"] = {\n",
        "            \"korean\": f\"{mood['korean_keyword']}, {detail1['korean_keyword']}, {detail2['korean_keyword']}\",\n",
        "            \"english\": f\"{mood['english_keyword']}, {detail1['english_keyword']}, {detail2['english_keyword']}\",\n",
        "            \"components\": {\n",
        "                \"mood\": mood,\n",
        "                \"detail1\": detail1,\n",
        "                \"detail2\": detail2\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # ìº¡ì…˜ íƒ€ì… 2: í†¤ 1ê°œ + ë©”ì´í¬ì—… ë””í…Œì¼ 2ê°œ\n",
        "    if \"í†¤\" in category_results and len(top_2_details) >= 2:\n",
        "        tone = category_results[\"í†¤\"]\n",
        "        detail1 = top_2_details[0]\n",
        "        detail2 = top_2_details[1]\n",
        "\n",
        "        captions[\"caption_type2\"] = {\n",
        "            \"korean\": f\"{tone['korean_keyword']}, {detail1['korean_keyword']}, {detail2['korean_keyword']}\",\n",
        "            \"english\": f\"{tone['english_keyword']}, {detail1['english_keyword']}, {detail2['english_keyword']}\",\n",
        "            \"components\": {\n",
        "                \"tone\": tone,\n",
        "                \"detail1\": detail1,\n",
        "                \"detail2\": detail2\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return captions\n",
        "\n",
        "# ====== ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ======\n",
        "def process_all_celebrity_images():\n",
        "    print(\"ğŸš€ í•œêµ­ì¸ ì–¼êµ´ ë©”ì´í¬ì—… ìº¡ì…˜ ìƒì„± ì‹œì‘!\")\n",
        "\n",
        "    # 1. BLIP2 ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    processor, model, device = initialize_blip2_model()\n",
        "\n",
        "    # 2. ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘\n",
        "    print(f\"\\nğŸ“¸ ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...\")\n",
        "    image_paths = collect_all_image_paths(image_base_dir)\n",
        "    total_images = len(image_paths)\n",
        "\n",
        "    print(f\"ğŸ“Š ì´ ë°œê²¬ëœ ì´ë¯¸ì§€: {total_images}ê°œ\")\n",
        "\n",
        "    # ì—°ì˜ˆì¸ë³„ ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸\n",
        "    celeb_count = {}\n",
        "    for path in image_paths:\n",
        "        celeb_name = os.path.basename(os.path.dirname(path))\n",
        "        celeb_count[celeb_name] = celeb_count.get(celeb_name, 0) + 1\n",
        "\n",
        "    print(f\"ğŸ‘¥ ì´ ì—°ì˜ˆì¸ ìˆ˜: {len(celeb_count)}ëª…\")\n",
        "\n",
        "    # 3. ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬\n",
        "    all_results = []\n",
        "\n",
        "    print(f\"\\nğŸ’„ ìº¡ì…˜ ìƒì„± ì‹œì‘ (ì „ì²´ {total_images}ê°œ ì´ë¯¸ì§€)\")\n",
        "\n",
        "    for i, image_path in enumerate(tqdm(image_paths, desc=\"ìº¡ì…˜ ìƒì„± ì¤‘\")):\n",
        "        try:\n",
        "            # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "            # ì¹´í…Œê³ ë¦¬ë³„ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            category_results = calculate_category_similarities(\n",
        "                image, processor, model, device, categories, keyword_translation\n",
        "            )\n",
        "\n",
        "            # ìº¡ì…˜ ìƒì„±\n",
        "            captions = generate_makeup_captions(category_results)\n",
        "\n",
        "            # ê²°ê³¼ ì €ì¥\n",
        "            result = {\n",
        "                \"index\": i,\n",
        "                \"image_path\": image_path,\n",
        "                \"image_name\": os.path.basename(image_path),\n",
        "                \"celebrity_name\": os.path.basename(os.path.dirname(image_path)),\n",
        "                \"category_results\": category_results,\n",
        "                \"captions\": captions\n",
        "            }\n",
        "            all_results.append(result)\n",
        "\n",
        "            # ë°°ì¹˜ ì €ì¥ (batch_sizeê°œë§ˆë‹¤)\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                batch_num = (i + 1) // batch_size\n",
        "                batch_save_path = os.path.join(output_dir, f\"batch_{batch_num:03d}_results.json\")\n",
        "\n",
        "                # í˜„ì¬ ë°°ì¹˜ë§Œ ì €ì¥\n",
        "                current_batch = all_results[-batch_size:]\n",
        "\n",
        "                with open(batch_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(current_batch, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                print(f\"\\nğŸ’¾ ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ ({i + 1}/{total_images}, {((i + 1)/total_images)*100:.1f}%)\")\n",
        "\n",
        "                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}, {os.path.basename(image_path)}): {e}\")\n",
        "            continue\n",
        "\n",
        "    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ (ë‚¨ì€ ì´ë¯¸ì§€ë“¤)\n",
        "    if len(all_results) % batch_size != 0:\n",
        "        final_batch_num = (len(all_results) // batch_size) + 1\n",
        "        final_batch_path = os.path.join(output_dir, f\"batch_{final_batch_num:03d}_final.json\")\n",
        "\n",
        "        remaining_results = all_results[-(len(all_results) % batch_size):]\n",
        "\n",
        "        with open(final_batch_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(remaining_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ ìµœì¢… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {final_batch_path}\")\n",
        "\n",
        "    # 4. ìµœì¢… í†µí•© ê²°ê³¼ ì €ì¥\n",
        "    final_save_path = os.path.join(output_dir, \"all_celebrity_makeup_captions.json\")\n",
        "    with open(final_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # 5. í†µê³„ ìƒì„±\n",
        "    stats = {\n",
        "        \"total_processed\": len(all_results),\n",
        "        \"successful_caption_type1\": sum(1 for r in all_results if \"caption_type1\" in r.get(\"captions\", {})),\n",
        "        \"successful_caption_type2\": sum(1 for r in all_results if \"caption_type2\" in r.get(\"captions\", {})),\n",
        "        \"celebrities\": celeb_count,\n",
        "        \"total_categories\": len(categories),\n",
        "        \"total_keywords\": sum(len(keywords) for keywords in categories.values())\n",
        "    }\n",
        "\n",
        "    stats_save_path = os.path.join(output_dir, \"processing_statistics.json\")\n",
        "    with open(stats_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # 6. ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"\\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“Š ì´ ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(all_results)}ê°œ\")\n",
        "    print(f\"ğŸ“Š ìº¡ì…˜ íƒ€ì…1 ì„±ê³µ: {stats['successful_caption_type1']}ê°œ\")\n",
        "    print(f\"ğŸ“Š ìº¡ì…˜ íƒ€ì…2 ì„±ê³µ: {stats['successful_caption_type2']}ê°œ\")\n",
        "    print(f\"ğŸ’¾ ìµœì¢… ê²°ê³¼: {final_save_path}\")\n",
        "    print(f\"ğŸ“ˆ í†µê³„ íŒŒì¼: {stats_save_path}\")\n",
        "\n",
        "    # 7. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
        "    if all_results:\n",
        "        print(f\"\\nğŸ‘€ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ì²« 3ê°œ):\")\n",
        "        for i, result in enumerate(all_results[:3]):\n",
        "            print(f\"\\nğŸ–¼ï¸ {i+1}. {result['image_name']} ({result['celebrity_name']})\")\n",
        "\n",
        "            if \"captions\" in result:\n",
        "                if \"caption_type1\" in result[\"captions\"]:\n",
        "                    caption1 = result[\"captions\"][\"caption_type1\"]\n",
        "                    print(f\"   ğŸ“ ìº¡ì…˜1: {caption1['korean']}\")\n",
        "                    print(f\"   ğŸ“ English: {caption1['english']}\")\n",
        "\n",
        "                if \"caption_type2\" in result[\"captions\"]:\n",
        "                    caption2 = result[\"captions\"][\"caption_type2\"]\n",
        "                    print(f\"   ğŸ“ ìº¡ì…˜2: {caption2['korean']}\")\n",
        "                    print(f\"   ğŸ“ English: {caption2['english']}\")\n",
        "\n",
        "    return all_results, stats\n",
        "\n",
        "# ====== ì‹¤í–‰ ======\n",
        "if __name__ == \"__main__\":\n",
        "    results, statistics = process_all_celebrity_images()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRLCzDH9JpXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìº¡ì…˜ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¾¸ê¸°"
      ],
      "metadata": {
        "id": "fNW-dCMBJy8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "input_dir = \"/content/drive/MyDrive/korean_celebrity_makeup_captions\"\n",
        "output_dir = \"/content/drive/MyDrive/á„á…¥á†«á„‘á…¥á„…á…¥á†«á„‰á…³/á„‰á…³á„á…¡á„‹á…µá†¯á„á…®á„á…¥á†«/á„’á…¡á†«á„€á…®á†¨á„‹á…µá†«á„‹á…¥á†¯á„€á…®á†¯/korean_celebrity_makeup_sentences\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ====== ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ë“¤ ======\n",
        "def make_caption_from_components(components):\n",
        "    \"\"\"ìº¡ì…˜ ì»´í¬ë„ŒíŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
        "    keywords = []\n",
        "\n",
        "    # ì»´í¬ë„ŒíŠ¸ì—ì„œ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
        "    for key, value in components.items():\n",
        "        if isinstance(value, dict) and 'english_keyword' in value:\n",
        "            keywords.append(value['english_keyword'])\n",
        "\n",
        "    if not keywords:\n",
        "        return \"\"\n",
        "\n",
        "    if len(keywords) == 1:\n",
        "        return f\"This makeup look features {keywords[0]}.\"\n",
        "    elif len(keywords) == 2:\n",
        "        return f\"This makeup look features {keywords[0]} and {keywords[1]}.\"\n",
        "    else:\n",
        "        return f\"This makeup look features {', '.join(keywords[:-1])}, and {keywords[-1]}.\"\n",
        "\n",
        "def make_korean_caption_from_components(components):\n",
        "    \"\"\"ìº¡ì…˜ ì»´í¬ë„ŒíŠ¸ë¥¼ í•œê¸€ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
        "    keywords = []\n",
        "\n",
        "    # ì»´í¬ë„ŒíŠ¸ì—ì„œ í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
        "    for key, value in components.items():\n",
        "        if isinstance(value, dict) and 'korean_keyword' in value:\n",
        "            keywords.append(value['korean_keyword'])\n",
        "\n",
        "    if not keywords:\n",
        "        return \"\"\n",
        "\n",
        "    if len(keywords) == 1:\n",
        "        return f\"ì´ ë©”ì´í¬ì—…ì€ {keywords[0]} ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤.\"\n",
        "    elif len(keywords) == 2:\n",
        "        return f\"ì´ ë©”ì´í¬ì—…ì€ {keywords[0]}ê³¼ {keywords[1]}ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"\n",
        "    else:\n",
        "        return f\"ì´ ë©”ì´í¬ì—…ì€ {', '.join(keywords[:-1])}, ê·¸ë¦¬ê³  {keywords[-1]}ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "# ====== ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ======\n",
        "def process_batch_files():\n",
        "    \"\"\"ë°°ì¹˜ íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë¬¸ì¥ ìƒì„±\"\"\"\n",
        "\n",
        "    print(\"ğŸ“ ë°°ì¹˜ íŒŒì¼ë“¤ì„ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ ì¤‘...\")\n",
        "\n",
        "    # batch_*.json íŒŒì¼ë“¤ ì°¾ê¸°\n",
        "    batch_files = []\n",
        "    for file_name in os.listdir(input_dir):\n",
        "        if file_name.startswith(\"batch_\") and file_name.endswith(\".json\"):\n",
        "            batch_files.append(file_name)\n",
        "\n",
        "    batch_files.sort()\n",
        "    print(f\"ğŸ“ ë°œê²¬ëœ ë°°ì¹˜ íŒŒì¼: {len(batch_files)}ê°œ\")\n",
        "\n",
        "    for file_name in batch_files:\n",
        "        input_path = os.path.join(input_dir, file_name)\n",
        "        output_path = os.path.join(output_dir, file_name.replace('.json', '_sentences.json'))\n",
        "\n",
        "        try:\n",
        "            with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # ê° ì´ë¯¸ì§€ ê²°ê³¼ì— ë¬¸ì¥ ì¶”ê°€\n",
        "            for item in data:\n",
        "                if \"captions\" in item:\n",
        "                    # ìº¡ì…˜ íƒ€ì…1 ë¬¸ì¥ ìƒì„±\n",
        "                    if \"caption_type1\" in item[\"captions\"]:\n",
        "                        components = item[\"captions\"][\"caption_type1\"].get(\"components\", {})\n",
        "                        item[\"captions\"][\"caption_type1\"][\"sentence_english\"] = make_caption_from_components(components)\n",
        "                        item[\"captions\"][\"caption_type1\"][\"sentence_korean\"] = make_korean_caption_from_components(components)\n",
        "\n",
        "                    # ìº¡ì…˜ íƒ€ì…2 ë¬¸ì¥ ìƒì„±\n",
        "                    if \"caption_type2\" in item[\"captions\"]:\n",
        "                        components = item[\"captions\"][\"caption_type2\"].get(\"components\", {})\n",
        "                        item[\"captions\"][\"caption_type2\"][\"sentence_english\"] = make_caption_from_components(components)\n",
        "                        item[\"captions\"][\"caption_type2\"][\"sentence_korean\"] = make_korean_caption_from_components(components)\n",
        "\n",
        "            # ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"âœ… {file_name} â†’ {os.path.basename(output_path)} ë³€í™˜ ì™„ë£Œ ({len(data)}ê°œ)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {file_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# ====== í†µí•© íŒŒì¼ ì²˜ë¦¬ ======\n",
        "def process_final_file():\n",
        "    \"\"\"ìµœì¢… í†µí•© íŒŒì¼ ì²˜ë¦¬\"\"\"\n",
        "\n",
        "    final_input_path = os.path.join(input_dir, \"all_celebrity_makeup_captions.json\")\n",
        "    final_output_path = os.path.join(output_dir, \"all_celebrity_makeup_sentences.json\")\n",
        "\n",
        "    if not os.path.exists(final_input_path):\n",
        "        print(\"âŒ í†µí•© íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nğŸ“ í†µí•© íŒŒì¼ ì²˜ë¦¬ ì¤‘...\")\n",
        "\n",
        "    try:\n",
        "        with open(final_input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # ê° ì´ë¯¸ì§€ ê²°ê³¼ì— ë¬¸ì¥ ì¶”ê°€\n",
        "        for item in data:\n",
        "            if \"captions\" in item:\n",
        "                # ìº¡ì…˜ íƒ€ì…1 ë¬¸ì¥ ìƒì„±\n",
        "                if \"caption_type1\" in item[\"captions\"]:\n",
        "                    components = item[\"captions\"][\"caption_type1\"].get(\"components\", {})\n",
        "                    item[\"captions\"][\"caption_type1\"][\"sentence_english\"] = make_caption_from_components(components)\n",
        "                    item[\"captions\"][\"caption_type1\"][\"sentence_korean\"] = make_korean_caption_from_components(components)\n",
        "\n",
        "                # ìº¡ì…˜ íƒ€ì…2 ë¬¸ì¥ ìƒì„±\n",
        "                if \"caption_type2\" in item[\"captions\"]:\n",
        "                    components = item[\"captions\"][\"caption_type2\"].get(\"components\", {})\n",
        "                    item[\"captions\"][\"caption_type2\"][\"sentence_english\"] = make_caption_from_components(components)\n",
        "                    item[\"captions\"][\"caption_type2\"][\"sentence_korean\"] = make_korean_caption_from_components(components)\n",
        "\n",
        "        # ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
        "        with open(final_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"âœ… í†µí•© íŒŒì¼ ë³€í™˜ ì™„ë£Œ: {final_output_path} ({len(data)}ê°œ)\")\n",
        "\n",
        "        # í†µê³„ ìƒì„±\n",
        "        type1_count = sum(1 for item in data if \"caption_type1\" in item.get(\"captions\", {}))\n",
        "        type2_count = sum(1 for item in data if \"caption_type2\" in item.get(\"captions\", {}))\n",
        "\n",
        "        print(f\"ğŸ“Š ìº¡ì…˜ íƒ€ì…1 ë¬¸ì¥: {type1_count}ê°œ\")\n",
        "        print(f\"ğŸ“Š ìº¡ì…˜ íƒ€ì…2 ë¬¸ì¥: {type2_count}ê°œ\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í†µí•© íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# ====== ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ======\n",
        "def show_sentence_samples(num_samples=5):\n",
        "    \"\"\"ìƒì„±ëœ ë¬¸ì¥ ìƒ˜í”Œ ë³´ê¸°\"\"\"\n",
        "\n",
        "    final_output_path = os.path.join(output_dir, \"all_celebrity_makeup_sentences.json\")\n",
        "\n",
        "    if not os.path.exists(final_output_path):\n",
        "        print(\"âŒ ë¬¸ì¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    with open(final_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"\\nğŸ‘€ ìƒì„±ëœ ë¬¸ì¥ ìƒ˜í”Œ ({num_samples}ê°œ):\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, item in enumerate(data[:num_samples]):\n",
        "        print(f\"\\nğŸ–¼ï¸ {i+1}. {item['image_name']} ({item['celebrity_name']})\")\n",
        "\n",
        "        if \"captions\" in item:\n",
        "            if \"caption_type1\" in item[\"captions\"]:\n",
        "                caption1 = item[\"captions\"][\"caption_type1\"]\n",
        "                print(f\"ğŸ“ íƒ€ì…1 (í•œê¸€): {caption1.get('sentence_korean', 'N/A')}\")\n",
        "                print(f\"ğŸ“ íƒ€ì…1 (ì˜ì–´): {caption1.get('sentence_english', 'N/A')}\")\n",
        "\n",
        "            if \"caption_type2\" in item[\"captions\"]:\n",
        "                caption2 = item[\"captions\"][\"caption_type2\"]\n",
        "                print(f\"ğŸ“ íƒ€ì…2 (í•œê¸€): {caption2.get('sentence_korean', 'N/A')}\")\n",
        "                print(f\"ğŸ“ íƒ€ì…2 (ì˜ì–´): {caption2.get('sentence_english', 'N/A')}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# ====== ë©”ì¸ ì‹¤í–‰ ======\n",
        "def convert_captions_to_sentences():\n",
        "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
        "\n",
        "    print(\"ğŸš€ ë©”ì´í¬ì—… ìº¡ì…˜ì„ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ ì‹œì‘!\")\n",
        "\n",
        "    # 1. ë°°ì¹˜ íŒŒì¼ë“¤ ì²˜ë¦¬\n",
        "    process_batch_files()\n",
        "\n",
        "    # 2. í†µí•© íŒŒì¼ ì²˜ë¦¬\n",
        "    data = process_final_file()\n",
        "\n",
        "    # 3. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
        "    if data:\n",
        "        show_sentence_samples(5)\n",
        "\n",
        "    print(f\"\\nğŸ‰ ë³€í™˜ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“‚ ì¶œë ¥ í´ë”: {output_dir}\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    convert_captions_to_sentences()"
      ],
      "metadata": {
        "id": "uXGJxT19JpVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ai pipelineì— ì¨ë‘” í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê¸°"
      ],
      "metadata": {
        "id": "p6oxLWsmJsb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "input_dir = \"/content/drive/MyDrive/korean_celebrity_makeup_captions\"\n",
        "output_dir = \"/content/drive/MyDrive/korean_celebrity_makeup_formatted\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ====== ë°ì´í„° ë³€í™˜ í•¨ìˆ˜ ======\n",
        "def convert_to_requested_format(item):\n",
        "    \"\"\"ê¸°ì¡´ ë°ì´í„°ë¥¼ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
        "\n",
        "    # request ì„¹ì…˜\n",
        "    request = {\n",
        "        \"ì´ë¯¸ì§€ê²½ë¡œ\": item.get(\"image_path\", \"\")\n",
        "    }\n",
        "\n",
        "    # response ì„¹ì…˜ ì´ˆê¸°í™”\n",
        "    response = {\n",
        "        \"predicted_keywords\": [],\n",
        "        \"caption\": \"\",\n",
        "        \"prompt_en\": \"\"\n",
        "    }\n",
        "\n",
        "    # ìº¡ì…˜ íƒ€ì…1 ìš°ì„  ì‚¬ìš© (ë¶„ìœ„ê¸° + ë©”ì´í¬ì—… ë””í…Œì¼ 2ê°œ)\n",
        "    captions = item.get(\"captions\", {})\n",
        "\n",
        "    if \"caption_type1\" in captions:\n",
        "        caption_data = captions[\"caption_type1\"]\n",
        "        components = caption_data.get(\"components\", {})\n",
        "\n",
        "        # predicted_keywords ìƒì„±\n",
        "        for comp_key, comp_value in components.items():\n",
        "            if isinstance(comp_value, dict):\n",
        "                keyword_entry = {\n",
        "                    \"keyword_kr\": comp_value.get(\"korean_keyword\", \"\"),\n",
        "                    \"keyword_en\": comp_value.get(\"english_keyword\", \"\"),\n",
        "                    \"score\": round(comp_value.get(\"score\", 0.0), 4)\n",
        "                }\n",
        "                response[\"predicted_keywords\"].append(keyword_entry)\n",
        "\n",
        "        # caption ìƒì„± (ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥)\n",
        "        keywords_en = [kw[\"keyword_en\"] for kw in response[\"predicted_keywords\"]]\n",
        "        if len(keywords_en) == 1:\n",
        "            response[\"caption\"] = f\"A woman with {keywords_en[0]} makeup.\"\n",
        "        elif len(keywords_en) == 2:\n",
        "            response[\"caption\"] = f\"A woman with {keywords_en[0]} and {keywords_en[1]} makeup.\"\n",
        "        elif len(keywords_en) >= 3:\n",
        "            response[\"caption\"] = f\"A woman with {', '.join(keywords_en[:-1])}, and {keywords_en[-1]} makeup.\"\n",
        "\n",
        "        # prompt_en ìƒì„± (ë©”ì´í¬ì—… ì¶”ì²œìš©)\n",
        "        if keywords_en:\n",
        "            response[\"prompt_en\"] = f\"{', '.join(keywords_en)} makeup, suitable for natural beauty.\"\n",
        "\n",
        "    # ìº¡ì…˜ íƒ€ì…1ì´ ì—†ìœ¼ë©´ íƒ€ì…2 ì‚¬ìš©\n",
        "    elif \"caption_type2\" in captions:\n",
        "        caption_data = captions[\"caption_type2\"]\n",
        "        components = caption_data.get(\"components\", {})\n",
        "\n",
        "        # predicted_keywords ìƒì„±\n",
        "        for comp_key, comp_value in components.items():\n",
        "            if isinstance(comp_value, dict):\n",
        "                keyword_entry = {\n",
        "                    \"keyword_kr\": comp_value.get(\"korean_keyword\", \"\"),\n",
        "                    \"keyword_en\": comp_value.get(\"english_keyword\", \"\"),\n",
        "                    \"score\": round(comp_value.get(\"score\", 0.0), 4)\n",
        "                }\n",
        "                response[\"predicted_keywords\"].append(keyword_entry)\n",
        "\n",
        "        # caption ìƒì„±\n",
        "        keywords_en = [kw[\"keyword_en\"] for kw in response[\"predicted_keywords\"]]\n",
        "        if len(keywords_en) == 1:\n",
        "            response[\"caption\"] = f\"A woman with {keywords_en[0]} makeup.\"\n",
        "        elif len(keywords_en) == 2:\n",
        "            response[\"caption\"] = f\"A woman with {keywords_en[0]} and {keywords_en[1]} makeup.\"\n",
        "        elif len(keywords_en) >= 3:\n",
        "            response[\"caption\"] = f\"A woman with {', '.join(keywords_en[:-1])}, and {keywords_en[-1]} makeup.\"\n",
        "\n",
        "        # prompt_en ìƒì„±\n",
        "        if keywords_en:\n",
        "            response[\"prompt_en\"] = f\"{', '.join(keywords_en)} makeup, suitable for natural beauty.\"\n",
        "\n",
        "    # ìµœì¢… í˜•íƒœ\n",
        "    result = {\n",
        "        \"request\": request,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# ====== ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ======\n",
        "def process_batch_files():\n",
        "    \"\"\"ë°°ì¹˜ íŒŒì¼ë“¤ì„ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
        "\n",
        "    print(\"ğŸ”„ ë°°ì¹˜ íŒŒì¼ë“¤ì„ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜ ì¤‘...\")\n",
        "\n",
        "    # batch_*.json íŒŒì¼ë“¤ ì°¾ê¸°\n",
        "    batch_files = []\n",
        "    for file_name in os.listdir(input_dir):\n",
        "        if file_name.startswith(\"batch_\") and file_name.endswith(\".json\"):\n",
        "            batch_files.append(file_name)\n",
        "\n",
        "    batch_files.sort()\n",
        "    print(f\"ğŸ“ ë°œê²¬ëœ ë°°ì¹˜ íŒŒì¼: {len(batch_files)}ê°œ\")\n",
        "\n",
        "    all_converted_data = []\n",
        "\n",
        "    for file_name in batch_files:\n",
        "        input_path = os.path.join(input_dir, file_name)\n",
        "\n",
        "        try:\n",
        "            with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # ê° ì´ë¯¸ì§€ ê²°ê³¼ë¥¼ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜\n",
        "            converted_batch = []\n",
        "            for item in data:\n",
        "                converted_item = convert_to_requested_format(item)\n",
        "                converted_batch.append(converted_item)\n",
        "                all_converted_data.append(converted_item)\n",
        "\n",
        "            # ë³€í™˜ëœ ë°°ì¹˜ ì €ì¥\n",
        "            output_path = os.path.join(output_dir, file_name.replace('.json', '_formatted.json'))\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(converted_batch, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"âœ… {file_name} â†’ {os.path.basename(output_path)} ë³€í™˜ ì™„ë£Œ ({len(converted_batch)}ê°œ)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {file_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    return all_converted_data\n",
        "\n",
        "# ====== í†µí•© íŒŒì¼ ì²˜ë¦¬ ======\n",
        "def process_final_file():\n",
        "    \"\"\"ìµœì¢… í†µí•© íŒŒì¼ì„ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
        "\n",
        "    final_input_path = os.path.join(input_dir, \"all_celebrity_makeup_captions.json\")\n",
        "    final_output_path = os.path.join(output_dir, \"all_celebrity_makeup_formatted.json\")\n",
        "\n",
        "    if not os.path.exists(final_input_path):\n",
        "        print(\"âŒ í†µí•© íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nğŸ”„ í†µí•© íŒŒì¼ ì²˜ë¦¬ ì¤‘...\")\n",
        "\n",
        "    try:\n",
        "        with open(final_input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # ê° ì´ë¯¸ì§€ ê²°ê³¼ë¥¼ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜\n",
        "        converted_data = []\n",
        "        for item in data:\n",
        "            converted_item = convert_to_requested_format(item)\n",
        "            converted_data.append(converted_item)\n",
        "\n",
        "        # ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
        "        with open(final_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"âœ… í†µí•© íŒŒì¼ ë³€í™˜ ì™„ë£Œ: {final_output_path} ({len(converted_data)}ê°œ)\")\n",
        "\n",
        "        # í†µê³„ ìƒì„±\n",
        "        valid_count = sum(1 for item in converted_data if item[\"response\"][\"predicted_keywords\"])\n",
        "        avg_keywords = sum(len(item[\"response\"][\"predicted_keywords\"]) for item in converted_data) / len(converted_data) if converted_data else 0\n",
        "\n",
        "        print(f\"ğŸ“Š ìœ íš¨í•œ ìº¡ì…˜: {valid_count}ê°œ\")\n",
        "        print(f\"ğŸ“Š í‰ê·  í‚¤ì›Œë“œ ìˆ˜: {avg_keywords:.1f}ê°œ\")\n",
        "\n",
        "        return converted_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í†µí•© íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "        return []\n",
        "\n",
        "# ====== ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ======\n",
        "def show_formatted_samples(num_samples=3):\n",
        "    \"\"\"ë³€í™˜ëœ ê²°ê³¼ ìƒ˜í”Œ ë³´ê¸°\"\"\"\n",
        "\n",
        "    final_output_path = os.path.join(output_dir, \"all_celebrity_makeup_formatted.json\")\n",
        "\n",
        "    if not os.path.exists(final_output_path):\n",
        "        print(\"âŒ ë³€í™˜ëœ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    with open(final_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"\\nğŸ‘€ ë³€í™˜ëœ ê²°ê³¼ ìƒ˜í”Œ ({num_samples}ê°œ):\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    for i, item in enumerate(data[:num_samples]):\n",
        "        print(f\"\\nğŸ“ ìƒ˜í”Œ {i+1}:\")\n",
        "        print(json.dumps(item, indent=2, ensure_ascii=False))\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# ====== ë‹¨ì¼ ìƒ˜í”Œ ì¶œë ¥ (ìš”ì²­ëœ ì •í™•í•œ í˜•íƒœ) ======\n",
        "def show_single_sample():\n",
        "    \"\"\"ìš”ì²­ëœ ì •í™•í•œ í˜•íƒœì˜ ë‹¨ì¼ ìƒ˜í”Œ ì¶œë ¥\"\"\"\n",
        "\n",
        "    final_output_path = os.path.join(output_dir, \"all_celebrity_makeup_formatted.json\")\n",
        "\n",
        "    if not os.path.exists(final_output_path):\n",
        "        print(\"âŒ ë³€í™˜ëœ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    with open(final_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if data:\n",
        "        print(f\"\\nğŸ“ ìš”ì²­ëœ í˜•íƒœì˜ ìƒ˜í”Œ:\")\n",
        "        print(json.dumps(data[0], indent=2, ensure_ascii=False))\n",
        "\n",
        "# ====== ê²€ì¦ í•¨ìˆ˜ ======\n",
        "def validate_format(data_sample):\n",
        "    \"\"\"ìš”ì²­ëœ í˜•íƒœì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦\"\"\"\n",
        "\n",
        "    required_structure = {\n",
        "        \"request\": [\"ì´ë¯¸ì§€ê²½ë¡œ\"],\n",
        "        \"response\": [\"predicted_keywords\", \"caption\", \"prompt_en\"]\n",
        "    }\n",
        "\n",
        "    keyword_structure = [\"keyword_kr\", \"keyword_en\", \"score\"]\n",
        "\n",
        "    # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦\n",
        "    for section, fields in required_structure.items():\n",
        "        if section not in data_sample:\n",
        "            return False, f\"'{section}' ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        for field in fields:\n",
        "            if field not in data_sample[section]:\n",
        "                return False, f\"'{section}.{field}' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    # keywords êµ¬ì¡° ê²€ì¦\n",
        "    keywords = data_sample[\"response\"][\"predicted_keywords\"]\n",
        "    if not isinstance(keywords, list):\n",
        "        return False, \"predicted_keywordsê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤.\"\n",
        "\n",
        "    for i, keyword in enumerate(keywords):\n",
        "        for field in keyword_structure:\n",
        "            if field not in keyword:\n",
        "                return False, f\"keyword[{i}]ì— '{field}' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    return True, \"í˜•íƒœê°€ ì˜¬ë°”ë¦…ë‹ˆë‹¤.\"\n",
        "\n",
        "# ====== ë©”ì¸ ì‹¤í–‰ ======\n",
        "def convert_to_requested_format_main():\n",
        "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
        "\n",
        "    print(\"ğŸš€ ë©”ì´í¬ì—… ìº¡ì…˜ì„ ìš”ì²­ëœ í˜•íƒœë¡œ ë³€í™˜ ì‹œì‘!\")\n",
        "\n",
        "    # 1. ë°°ì¹˜ íŒŒì¼ë“¤ ì²˜ë¦¬\n",
        "    all_data = process_batch_files()\n",
        "\n",
        "    # 2. í†µí•© íŒŒì¼ ì²˜ë¦¬\n",
        "    final_data = process_final_file()\n",
        "\n",
        "    # 3. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
        "    if final_data:\n",
        "        show_single_sample()\n",
        "\n",
        "        # 4. í˜•íƒœ ê²€ì¦\n",
        "        is_valid, message = validate_format(final_data[0])\n",
        "        print(f\"\\nâœ… í˜•íƒœ ê²€ì¦: {message}\")\n",
        "\n",
        "    print(f\"\\nğŸ‰ ë³€í™˜ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“‚ ì¶œë ¥ í´ë”: {output_dir}\")\n",
        "    print(f\"ğŸ“ ì£¼ìš” íŒŒì¼: all_celebrity_makeup_formatted.json\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    convert_to_requested_format_main()"
      ],
      "metadata": {
        "id": "BiVfBLrXJpTg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}